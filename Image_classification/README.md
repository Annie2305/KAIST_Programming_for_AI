# ğŸ§  FashionMNIST Classification with ResNet (AI504 Project 1)

This repository contains my implementation of a **ResNet-based CNN model** for the **FashionMNIST** classification task, as part of **KAIST AI504: Programming for AI (Fall 2025)**.

---

## ğŸš€ Project Overview

The objective of this project was to design, train, and evaluate a **CNN** on the FashionMNIST dataset, achieving at least **92.43% validation accuracy**.

I implemented a **ResNet-style architecture completely from scratch** using PyTorch â€” without relying on any pretrained model or built-in ResNet class.  
The model successfully **outperformed the baseline**, reaching **93.21% validation accuracy** ğŸ¯.

---

## âœ¨ Key Improvements Beyond Baseline

| Improvement | Description | Impact |
|--------------|--------------|---------|
| ğŸ§© **Custom ResNet Implementation** | Built a simplified residual network manually (BasicBlock + skip connection + BN). | Improved training stability and gradient flow. |
| ğŸ¨ **Data Augmentation** | Added `RandomCrop`, `RandomHorizontalFlip`, and `RandomRotation` to increase dataset diversity. | Reduced overfitting, better generalization. |
| âš™ï¸ **Advanced Optimizer â€” AdamW** | Switched from standard Adam to **AdamW** with weight decay. | Improved regularization and final accuracy (+0.5â€“0.8%). |
| ğŸ“‰ **Label Smoothing in CrossEntropy** | Applied `label_smoothing=0.1` to prevent overconfidence. | Smoother training and more stable validation accuracy. |
| ğŸŒŠ **Cosine Annealing LR Scheduler** | Gradually decayed learning rate for smoother convergence. | Boosted stability and fine-tuning in later epochs. |

---

## ğŸ§© Model Architecture

### ğŸ”¹ BasicBlock
Each block contains:
- Two 3Ã—3 convolution layers  
- Batch Normalization after each  
- ReLU activation  
- Skip connection with optional 1Ã—1 conv (for downsampling)

### ğŸ”¹ Custom Small ResNet
| Layer | Output Channels | Stride | #Blocks |
|--------|------------------|---------|----------|
| Conv1  | 32 | 1 | - |
| Layer1 | 32 | 1 | 2 |
| Layer2 | 64 | 2 | 2 |
| Layer3 | 128 | 2 | 2 |
| AvgPool + FC | - | - | - |

---

## ğŸ§  Training Configuration

| Setting | Value |
|----------|--------|
| Optimizer | AdamW |
| Learning Rate | 3e-4 |
| Weight Decay | 1e-4 |
| LR Scheduler | CosineAnnealingLR (T_max=40) |
| Loss Function | CrossEntropyLoss (label_smoothing=0.1) |
| Epochs | 40 |
| Batch Size | 128 |
| Data Augmentation | RandomCrop(28,2), RandomHorizontalFlip(0.5), RandomRotation(10Â°) |
| Normalization | Mean = 0.5, Std = 0.5 |

---

## ğŸ“Š Results

| Metric | Value |
|---------|--------|
| **Best Validation Accuracy** | **93.21%** âœ… |
| **Baseline Accuracy** | 92.43% |
| **Improvement** | **+0.78%** |
| Test Logits Shape | (10000, 10) |
| Saved Logits | `logits.npy` |

