# -*- coding: utf-8 -*-
"""20256307.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aDkH_dUzH9jtLbJHQLhJpAmd1LW198cH
"""

import os
import random
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# Vision utilities
import torchvision
from torchvision import datasets, transforms, models

# Data handling
from torch.utils.data import DataLoader, random_split

# Device & reproducibility
from torch.backends import cudnn

# Optional but handy
from tqdm import tqdm   # for progress bars

def set_seed(seed=0):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

def main():
    set_seed(0)

    if torch.backends.mps.is_available():
        device = torch.device("mps")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")

    print("Using device:", device)

    # ----------------------------Load the data---------------------------- #

    # Define a transform to normalize the data
    transform = transforms.Compose([transforms.RandomCrop(28, padding=2),
                                    transforms.RandomHorizontalFlip(p=0.5),
                                    transforms.RandomRotation(10),
                                    transforms.ToTensor(),
                                    transforms.Normalize((0.5,), (0.5,))])

    trainset = datasets.FashionMNIST('./data', download=True, train=True, transform=transform)
    valset = int(len(trainset) * 0.1)
    train_set, val_set = random_split(trainset, [len(trainset) - valset, valset])

    testset = datasets.FashionMNIST('./data', download=True, train=False, transform=transform)

    valloader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False)
    trainloader = torch.utils.data.DataLoader(train_set, batch_size=128, shuffle=True)
    testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False)

    print(f"Train samples: {len(train_set)}, Val samples: {len(val_set)}, Test samples: {len(testset)}")


    # ----------------------------Model Definition---------------------------- #
    class BasicBlock(nn.Module):
        def __init__(self, in_channels, out_channels, stride=1):
            super().__init__()
            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size = 3, stride = stride, padding = 1, bias = False)
            self.bn1 = nn.BatchNorm2d(out_channels)
            self.relu = nn.ReLU()
            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size = 3, stride = 1, padding = 1, bias = False)
            self.bn2 = nn.BatchNorm2d(out_channels)

            self.downsample = None
            if stride != 1 or in_channels != out_channels:
                self.downsample = nn.Sequential(
                    nn.Conv2d(in_channels, out_channels, kernel_size = 1, stride = stride, bias = False),
                    nn.BatchNorm2d(out_channels))
            self.stride = stride

        def forward(self, x):
            identity = x
            out = self.conv1(x)
            out = self.bn1(out)
            out = self.relu(out)

            out = self.conv2(out)
            out = self.bn2(out)

            if self.downsample is not None:
                identity = self.downsample(identity)
            out += identity
            out = self.relu(out)
            return out

    class ResNet(nn.Module):
        def __init__(self, num_classes=10):
            super().__init__()
            self.conv1 = nn.Conv2d(1, 32, kernel_size = 3, stride = 1, padding = 1, bias = False)
            self.bn1 = nn.BatchNorm2d(32)
            self.relu = nn.ReLU()

            self.make_layer1 = self._make_layer(32, 32, blocks = 2, stride = 1)
            self.make_layer2 = self._make_layer(32, 64, blocks = 2, stride = 2)
            self.make_layer3 = self._make_layer(64, 128, blocks = 2, stride = 2)

            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(128, num_classes)

        def _make_layer(self, in_channels, out_channels, blocks, stride):
            layers = []
            layers.append(BasicBlock(in_channels, out_channels, stride))
            for _ in range(1, blocks):
                layers.append(BasicBlock(out_channels, out_channels, stride = 1))
            return nn.Sequential(*layers)

        def forward(self, x):
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.relu(x)
            x = self.make_layer1(x)
            x = self.make_layer2(x)
            x = self.make_layer3(x)
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.fc(x)
            return x



    # ----------------------------Training Loop---------------------------- #

    model = ResNet(num_classes = 10).to(device)

    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.AdamW(model.parameters(), lr = 3e-4, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40)

    best_acc = 0.0
    num_epochs = 40
    for epoch in range(num_epochs):
        running_loss = 0.0
        model.train()
        for images, labels in tqdm(trainloader):
            images, labels = images.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        model.eval()
        with torch.no_grad():
            correct = 0
            total = 0
            for images, labels in valloader:
                images, labels = images.to(device), labels.to(device)
                outputs = model(images)
                _, predicted = torch.max(outputs.data, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()
        acc = 100*correct/total
        if acc > best_acc:
            best_acc = acc
            torch.save(model.state_dict(), 'best_model.pth')

        print(
            f"Epoch {epoch+1}/{num_epochs}, "
            f"Loss: {running_loss/len(trainloader):.4f}, "
            f"Val Accuracy: {acc:.2f}%, "
            f"Best: {best_acc:.2f}%"
            )


    # ---------- Final Test Inference & Logits Export --------- #

    model.load_state_dict(torch.load("best_model.pth", map_location=device))
    model.eval()

    all_logits = []
    with torch.no_grad():
        for images,_ in testloader:
            images = images.to(device)
            logits = model(images)
            all_logits.append(logits.cpu())

    logits_np = torch.cat(all_logits, dim=0).numpy().astype(np.float32)
    print("Logits shape:", logits_np.shape)  # should be (10000, 10)
    np.save("20256307.npy", logits_np)

if __name__ == "__main__":
    main()
