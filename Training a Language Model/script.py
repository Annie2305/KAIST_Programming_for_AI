# -*- coding: utf-8 -*-
"""20256307.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LmcKpDVX-4R8qTDXzr5o-pj1SzQfmVd6
"""

import random
import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset
from datasets import load_dataset
from transformers import AutoTokenizer
from transformers import GPT2LMHeadModel
from torch.optim import AdamW
import math
import torch.nn.functional as F
import torch.nn as nn

########################## MUST NOT MODIFY #########################
# Function to set random seed
def set_seed(seed=0):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


# Function for preprocessing data
class eli5_dataset(Dataset):
    def __init__(self,tokenizer, MAX_POSITION_EMBEDDINGS, data_type):
        super().__init__()

        self.tokenizer = tokenizer
        self.block_size = MAX_POSITION_EMBEDDINGS

        if data_type == "train":
            data = load_dataset("dany0407/eli5_category", split="train[:30000]")
            data = data.select(range(10000))
        elif data_type == "valid":
            data = load_dataset("dany0407/eli5_category", split="validation1[:2000]")
        elif data_type == "test":
            data = load_dataset("dany0407/eli5_category", split="test[:20]")

        data = data.flatten()
        data = data.map(self.preprocess_function, batched=True,num_proc=8,remove_columns=data.column_names)
        data = data.map(self.group_texts, batched=True, num_proc=8)
        result =[]
        for i in data:
            result.append(i['input_ids'])
        self.final_data = torch.tensor(result).to(torch.int64)

    def preprocess_function(self, examples):
        return self.tokenizer([" ".join(x) for x in examples["answers.text"]])

    def group_texts(self, examples):

        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}
        total_length = len(concatenated_examples[list(examples.keys())[0]])
        if total_length >= (self.block_size-2):
            total_length = (total_length // (self.block_size-2)) * (self.block_size-2)
        result = {
            k: [[self.tokenizer.bos_token_id]+t[i : i + self.block_size-2]+[self.tokenizer.eos_token_id] for i in range(0, total_length, self.block_size-2)]
            for k, t in concatenated_examples.items()
        }
        result["labels"] = result["input_ids"].copy()
        return result

    def __len__(self):
        return len(self.final_data)

    def __getitem__(self, idx):
        return self.final_data[idx]
#######################################################################

class MultiHeadAttention(nn.Module):
  def __init__(self, d_model=256, num_heads=4):
    super().__init__()

    self.d_model = d_model
    self.num_heads = num_heads
    self.head_dim = d_model // num_heads

    self.W_q = nn.Linear(d_model, d_model)
    self.W_k = nn.Linear(d_model, d_model)
    self.W_v = nn.Linear(d_model, d_model)

    self.W_o = nn.Linear(d_model, d_model)

  def forward(self, x, mask):
    batch, seq_len, _ = x.shape

    Q = self.W_q(x)
    K = self.W_k(x)
    V = self.W_v(x)

    Q = Q.reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
    K = K.reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
    V = V.reshape(batch, seq_len, self.num_heads, self.head_dim).transpose(1, 2)

    scores = (Q @ K.transpose(-2, -1)/math.sqrt(self.head_dim))

    scores = scores.masked_fill(mask == 0, float('-inf'))

    attn = torch.softmax(scores, dim = -1)

    out = attn @ V
    out = out.transpose(1, 2).reshape(batch, seq_len, self.d_model)

    out = self.W_o(out)

    return out

class FeedForward(nn.Module):
  def __init__(self, d_model = 256, ffn_dim = 1024):
    super().__init__()

    self.fc1 = nn.Linear(d_model, ffn_dim)
    self.fc2 = nn.Linear(ffn_dim, d_model)
    self.relu = nn.ReLU()
    self.dropout = nn.Dropout(0.1)

  def forward(self, x):
    x = self.fc1(x)
    x = self.relu(x)
    x = self.dropout(x)
    x = self.fc2(x)
    return x

class DecoderBlock(nn.Module):
  def __init__(self, d_model = 256, num_heads = 4, ffn_dim = 1024):
    super().__init__()

    self.ln1 = nn.LayerNorm(d_model)
    self.attn = MultiHeadAttention(d_model = d_model, num_heads = num_heads)
    self.ln2 = nn.LayerNorm(d_model)
    self.ffn = FeedForward(d_model=d_model, ffn_dim=ffn_dim)

  def forward(self, x, mask):
    x = x + self.attn(self.ln1(x), mask)

    x = x + self.ffn(self.ln2(x))

    return x

class TinyTransformerLM(nn.Module):
    def __init__(self, vocab_size=50257, max_seq_len=200,
                 d_model=256, num_heads=4, ffn_dim=1024, num_layers=2):
        super().__init__()

        self.token_emb = nn.Embedding(vocab_size, d_model)
        self.pos_emb = nn.Embedding(max_seq_len, d_model)

        # Stack decoder blocks
        self.layers = nn.ModuleList([
            DecoderBlock(d_model=d_model, num_heads=num_heads, ffn_dim=ffn_dim)
            for _ in range(num_layers)
        ])

        self.ln_f = nn.LayerNorm(d_model)
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

        self.max_seq_len = max_seq_len

    def forward(self, input_ids, labels=None):
        batch, seq_len = input_ids.shape
        device = input_ids.device

        # Embeddings
        positions = torch.arange(seq_len, device=device).unsqueeze(0)   # (1, seq_len)
        x = self.token_emb(input_ids) + self.pos_emb(positions)

        # Causal mask
        mask = torch.tril(torch.ones(seq_len, seq_len, device=device))
        mask = mask.unsqueeze(0).unsqueeze(0)   # (1, 1, seq_len, seq_len)

        # Pass through decoder blocks
        for block in self.layers:
            x = block(x, mask)

        # Final norm + LM head
        x = self.ln_f(x)
        logits = self.lm_head(x)

        # Loss
        if labels is not None:
            logits_flat = logits.reshape(-1, logits.size(-1))
            labels_flat = labels.reshape(-1)

            loss = F.cross_entropy(
                logits_flat,
                labels_flat,
                ignore_index=-100
            )
            return logits, loss

        return logits

def calculate_perplexity(logits, targets):
    loss = F.cross_entropy(
        logits.reshape(-1, logits.shape[-1]),
        targets.reshape(-1),
        reduction='mean'
    )
    return torch.exp(loss)


def main():
    ########################## MUST NOT MODIFY #########################
    # Set seed once at the top of your code (before executing any other code)
    set_seed(seed=0)

    # Preprocessing code
    tokenizer = AutoTokenizer.from_pretrained("gpt2")

    BATCH_SIZE = 32
    MAX_POSITION_EMBEDDINGS = 200
    trainset = eli5_dataset(tokenizer, MAX_POSITION_EMBEDDINGS, "train")
    validset = eli5_dataset(tokenizer, MAX_POSITION_EMBEDDINGS, "valid")
    testset = eli5_dataset(tokenizer, MAX_POSITION_EMBEDDINGS, "test")

    print(len(trainset)) # 17655
    print(len(validset)) # 5344
    print(len(testset)) # 75
    print(len(trainset[0])) # 200
    print(len(validset[0])) # 200
    print(len(testset[0])) # 200

    # Must use the provided train_dataloader, valid_dataloader, test_dataloader
    train_dataloader = DataLoader(trainset, batch_size=BATCH_SIZE)
    valid_dataloader = DataLoader(validset, batch_size=BATCH_SIZE)
    test_dataloader = DataLoader(testset, batch_size=BATCH_SIZE)
    #######################################################################

    device = torch.device('cuda' if torch.cuda.is_available () else 'cpu')

    model = TinyTransformerLM(
            vocab_size=50257,
            max_seq_len=200,
            d_model=320,
            num_heads=5,
            ffn_dim=1280,
            num_layers=4
            ).to(device)

    optimizer = AdamW(model.parameters(), lr = 5e-4)

    num_epochs = 6
    model.train()
    for epoch in range(num_epochs):

        # ---------------- TRAINING ----------------
        for batch in train_dataloader:
            batch = batch.to(device)

            logits = model(batch)  # forward

            # SHIFTED LOSS
            loss = F.cross_entropy(
                logits[:, :-1].reshape(-1, logits.size(-1)),
                batch[:, 1:].reshape(-1)
            )

            loss.backward()
            optimizer.step()
            optimizer.zero_grad()

        # ---------------- VALIDATION ----------------
        model.eval()
        val_ppls = []
        with torch.no_grad():
            for batch in valid_dataloader:
                batch = batch.to(device)

                logits = model(batch)
                ppl = calculate_perplexity(
                    logits[:, :-1],
                    batch[:, 1:]
                )
                val_ppls.append(ppl.item())

        avg_val_ppl = sum(val_ppls) / len(val_ppls)
        print(f"Epoch {epoch+1} | Val PPL: {avg_val_ppl:.4f}")

        model.train()

    # ---------------- TEST LOGITS GENERATION ----------------
    model.eval()
    all_logits = []

    with torch.no_grad():
        for batch in test_dataloader:
            batch = batch.to(device)
            logits = model(batch)
            all_logits.append(logits.cpu())

    all_logits = torch.cat(all_logits, dim=0)       # (75, 200, vocab)
    logits_np = all_logits.numpy().astype(np.float16)
    np.save('20256307.npy', logits_np)

if __name__ == "__main__":
    main()
